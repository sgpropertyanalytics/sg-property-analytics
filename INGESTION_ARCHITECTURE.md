# Data Ingestion & Scraping Architecture

> **Core Principle:** One ingestion pipeline. One source of truth. All changes auditable.

---

## 1. System Purpose

This system ingests, validates, and consolidates property market data from multiple sources into structured domain tables for analytics and visualization.

**Designed to:**
- Track provenance
- Detect schema drift
- Prevent silent data corruption
- Support future multi-source reconciliation
- Maintain auditability and reproducibility

**Not optimized for:**
- Maximum scraping speed
- Blind automation
- High-frequency scraping

---

## 2. Data Sources

### Scraped Sources

| Data | Source | Method |
|------|--------|--------|
| GLS Tenders | URA.gov.sg | Web scraping |
| Geocoding | OneMap API | API |

> **Note:** GLS is the only scraped dataset.

### File-Based Sources (Not Scraped)

| Data | Source | Method |
|------|--------|--------|
| Transactions | URA REALIS | CSV upload (ETL) |
| Total Units | Manual CSV | File-based |
| Upcoming Launches | Manual CSV | File-based |

---

## 3. Unit Data Model

Unit counts use a 3-tier confidence system:

| Tier | Source | Confidence |
|------|--------|------------|
| 1 | Manual CSV (`new_launch_units.csv`) | High |
| 2 | `upcoming_launches` table | Medium |
| 3 | Estimated from transactions | Low |

**This hierarchy is intentional and must be preserved.**

---

## 4. Architecture Overview

### A. Legacy Scraper (Production)

**File:** `services/gls_scraper.py`

**Responsibilities:**
- Fetch URA GLS pages
- Parse tender data
- Call OneMap API
- âŒ Writes directly to `gls_tenders`

**Limitations:**
- No diffing
- No audit trail
- No schema change detection
- No conflict detection

### B. Ingestion Orchestrator (Built, Activating)

**Location:** `scrapers/` (rename mentally to "ingestion orchestrator")

**Purpose:** A generalized ingestion framework responsible for:
- Run tracking
- Provenance
- Schema change detection
- Multi-source readiness
- Promotion gating
- Diff vs existing

**Key Components:**

| File | Purpose |
|------|---------|
| `orchestrator.py` | Execution controller |
| `tier_system.py` | Source trust levels |
| `field_authority.py` | Field ownership rules |
| `rate_limiter.py` | Domain-keyed rate limiting |
| `adapters/ura_gls.py` | Wraps legacy scraper |
| `promoters/` | Domain writers |
| `utils/schema_diff.py` | Structure change detection |
| `utils/hashing.py` | Content fingerprinting |

**Tracking Tables:**
- `scrape_runs` â†’ `ingestion_runs`
- `scraped_entities`
- `canonical_entities`
- `entity_candidates`
- `scraper_schema_changes`
- `discovered_links`

---

## 5. Current State

| Area | Status |
|------|--------|
| Legacy scraper | âœ… Active |
| Orchestrator infrastructure | âœ… Built |
| Orchestrator in production | âŒ Not yet |
| Diff vs existing | âŒ Missing |
| Conflict detection | âŒ Missing |
| Promotion gating | âŒ Missing |
| Multi-source ingestion | âŒ Not used |

> âš ï¸ Two systems exist, but only the old one writes to production.

---

## 6. Core Design Rule (NON-NEGOTIABLE)

**Only the orchestrator may write to domain tables** (`gls_tenders`, `upcoming_launches`).

The scraper/uploader:
- âœ… May fetch and parse data
- âœ… May enrich with geocoding
- âŒ Must not write production tables directly

---

## 7. Correct Ingestion Flow

```
Scraper / CSV / API
       â†“
scraped_entities (raw extraction)
       â†“
canonical_entities (merged truth)
       â†“
diff + reconciliation
       â†“
promotion gate (block on conflicts)
       â†“
domain tables (gls_tenders, upcoming_launches, transactions)
```

---

## 8. Required Capabilities (To Implement)

### Diff Detection
Every ingestion must output:
- `unchanged` â€” no change from existing
- `changed` â€” values differ (list fields)
- `new` â€” record doesn't exist yet
- `missing` â€” record exists but not in source (deletion candidate)

### Conflict Detection
Flag suspicious changes:
- Value swings > threshold (e.g., units changed by >50%)
- Regressions (awarded â†’ launched)
- Invalid state transitions
- Null â†’ value where shouldn't happen

### Promotion Gating
- Block on hard conflicts
- Configurable thresholds
- Require approval for suspicious changes
- Auto-promote clean records

### Audit Output
- Per-run diff reports
- `run_id` on every change
- `who/what triggered it`
- Reproducibility

---

## 9. Multi-Source Policy

Multi-source scraping (PropertyGuru, 99.co, etc.) is:
- âŒ NOT active
- âš ï¸ NOT required yet
- âœ… Architecturally supported

If added:
- Treated as Tier B/C
- Never auto-promoted
- Routed to `entity_candidates`
- Requires manual approval

---

## 10. Verification Mode for File-Based Datasets

### Purpose
CSV values were historically scraped and may be inaccurate. Periodic verification ensures data quality.

### Datasets Requiring Verification
- `new_launch_units.csv` (total units)
- `upcoming_launches.csv` (launch schedule + metadata)
- Derived "units from transactions" estimates

### Verification Sources + Trust Tiers

| Tier | Sources | Use |
|------|---------|-----|
| **A** (highest) | URA, official docs, developer PDFs, official project pages | Authoritative |
| **B** | PropertyGuru, 99.co, EdgeProp | Cross-validation |
| **C** | Blogs, forums | Never used to auto-correct |

### Cross-Validation Logic

#### A) Total Units (`new_launch_units.csv`)

For each project:
1. Check "units" from Tier A/B sources if available
2. Cross-check internal consistency:
   - If `transactions_count > total_units` â†’ **hard conflict**
   - If units suspiciously low/high â†’ **warning**
3. Compare CSV value vs verification value:
   - Exact match â†’ âœ… `CONFIRMED`
   - Mismatch â†’ âš ï¸ `CONFLICT` (requires approval)
   - Cannot verify â†’ ğŸŸ¨ `UNVERIFIED` (keep CSV)

**Output fields:**
```
units_verified_value
units_verified_sources[]
units_verification_status = CONFIRMED | CONFLICT | UNVERIFIED
units_last_verified_at
units_last_verified_run_id
```

#### B) Upcoming Launches

Verify:
- Project exists (name/alias mapping)
- Launch month/quarter plausibility
- Status transitions (rumoured â†’ preview â†’ launched â†’ sold out)
- Cross-check vs GLS tender linkage

### Promotion Policy

Verification results **do not directly overwrite** CSV or domain tables.

Instead:
1. Write to `entity_candidates` (or `verification_candidates`) with:
   - `before` / `after` values
   - `sources[]`
   - `confidence_score`
   - `recommended_action`
2. Only after approval (manual or rule-based) update:
   - `new_launch_units.csv` (or migrate to DB table)
   - `upcoming_launches` table

### Run Cadence

| Frequency | Scope |
|-----------|-------|
| **Weekly** | Sample (top N active / most-viewed / recently transacted) |
| **Monthly** | Full sweep of all projects in CSV |

**Trigger conditions for immediate check:**
- Project appears in transactions but has missing/low-confidence units
- Transaction volume spikes
- New GLS tender awarded / new launch introduced

### Verification Report Format

```
Verification Report
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Confirmed: X
âš ï¸ Mismatch candidates: Y
â€¼ï¸ Hard conflicts: Z
ğŸŸ¨ Unverified: W

Top Issues:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Project     â”‚ Current â”‚ Suggested â”‚ Sources     â”‚ Confidence â”‚ Action      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ THE INTERLACEâ”‚ 1,040  â”‚ 1,040     â”‚ URA, PG     â”‚ HIGH       â”‚ CONFIRM     â”‚
â”‚ D'LEEDON    â”‚ 1,700   â”‚ 1,715     â”‚ EdgeProp    â”‚ MEDIUM     â”‚ REVIEW      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Implementation Roadmap

### Phase 1: Unified Ingestion Pattern
- [ ] Rename `scrape_runs` â†’ `ingestion_runs`
- [ ] Add `source_type` column (scrape, csv_upload, api)
- [ ] Add diff detection to orchestrator
- [ ] Add promotion gating logic

### Phase 2: Apply to All Pipelines
- [ ] GLS scrape â†’ orchestrator with diff
- [ ] URA REALIS ETL â†’ orchestrator with diff
- [ ] `upcoming_launches.csv` â†’ orchestrator with diff
- [ ] `new_launch_units.csv` â†’ orchestrator with diff

### Phase 3: Verification Mode âœ… COMPLETE
- [x] Create `verification_candidates` table (migration 012)
- [x] Implement cross-validation logic (`scrapers/utils/cross_validator.py`)
- [x] Add verification status fields to domain tables (migration 013)
- [x] Build verification report generator (`services/verification_report.py`)
- [x] Create 5 Tier B adapters (PropertyGuru, EdgeProp, 99.co, ERA, PropNex)
- [x] Build verification API routes (`/api/verification/*`)
- [x] Enforce 3-source minimum for auto-confirm

### Phase 4: UI Integration
- [ ] Show confidence badges on unit counts
- [ ] Review queue for conflicts
- [ ] Verification status indicators

---

## 12. File Reference

### Core Infrastructure
| File | Purpose |
|------|---------|
| `backend/scrapers/orchestrator.py` | Main ingestion controller |
| `backend/scrapers/tier_system.py` | Source trust levels |
| `backend/scrapers/field_authority.py` | Field ownership rules |
| `backend/services/gls_scraper.py` | Legacy GLS scraper |
| `backend/services/new_launch_units.py` | 3-tier unit lookup |

### Data Files
| File | Purpose |
|------|---------|
| `backend/data/new_launch_units.csv` | Manual unit counts |
| `backend/data/upcoming_launches.csv` | Launch schedule |

### Agents
| Agent | Purpose |
|-------|---------|
| `scraping-orchestrator.md` | Scraping/ingestion guidance |
| `etl-pipeline.md` | URA REALIS CSV processing |
| `data-integrity-validator.md` | Data quality checks |

---

## 13. Guidelines for All Future Changes

When proposing or implementing changes:

âœ… Respect the orchestrator as the sole write authority
âœ… Preserve provenance
âœ… Avoid duplicate write paths
âœ… Prefer reconciliation over overwrite
âŒ Do not add direct-write promotion logic
âŒ Do not bypass canonicalization

---

## 14. Long-Term Recommendation

Migrate from CSV files to database tables with proper audit fields:

```sql
CREATE TABLE project_reference (
    id SERIAL PRIMARY KEY,
    project_name VARCHAR(255) UNIQUE NOT NULL,
    total_units INTEGER,
    units_source VARCHAR(50),
    units_confidence VARCHAR(20),
    units_verified_at TIMESTAMP,
    units_verified_run_id VARCHAR(36),
    developer VARCHAR(255),
    tenure VARCHAR(50),
    district VARCHAR(10),
    -- ... other fields
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

This provides:
- Audit trail on every change
- Confidence tracking
- Verification status
- Run linkage

Keep CSV as import/export format, not source of truth.
